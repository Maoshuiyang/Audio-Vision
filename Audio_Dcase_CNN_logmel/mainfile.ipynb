{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCASE 2016 DNN Baseline\n",
    "In this notebook, we implement the **Detection and Classification of Acoustic Scenes and Events challenge** 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppress warnings**: We need to supress warnings as we are going to use some functionality of an older version of *scikit-learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras_aud library\n",
    "Clone [keras_aud](https://github.com/channelCS/keras_aud) and place the **path** in *ka_path* variable so that we can import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio Feature extraction script\n",
      "Script by channelCS"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "ERROR (theano.gpuarray): pygpu was configured but could not be imported or is too old (version 0.6 or higher required)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda2\\lib\\site-packages\\theano\\gpuarray\\__init__.py\", line 23, in <module>\n",
      "    import pygpu.version\n",
      "ImportError: No module named version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "ka_path=\"C:/Users/aditya/version-control\"\n",
    "#ka_path=\"E:/akshita_workspace/cc\"\n",
    "sys.path.insert(0, ka_path)\n",
    "from keras_aud import aud_audio, aud_feature\n",
    "from keras_aud import aud_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make imports**: We now import libraries which shall be required in this task. We use\n",
    "1. `csv` for reading `.csv` files.\n",
    "2. `cPickle` for reading `.f` pickle files.\n",
    "3. `scipy` for calculating `mode`\n",
    "4. `time` for calciulating *time to load* pickle files.\n",
    "5. `KFold` for kfold cross validation.\n",
    "6. `to_categorical` for reshaping *labels* into `num_classes`.\n",
    "7. `load_model` for loading a saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cPickle\n",
    "import numpy as np\n",
    "import scipy\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define paths**: We now tell the paths for **audio**, **features** and **texts**.\n",
    "\n",
    "| Variable        | Description                     |\n",
    "| :-------------  |:-------------                   |\n",
    "| `wav_dev_fd`    | Development audio folder        |\n",
    "| `wav_eva_fd`    | Evaluation audio folder         |\n",
    "| `dev_fd`        | Development features folder     |\n",
    "| `eva_fd`        | Evaluation features folder      |\n",
    "| `label_csv`     | Development meta file           |\n",
    "| `txt_eva_path`  | Evaluation test file            |\n",
    "| `new_p`         | Evaluation evaluate file        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wav_dev_fd   = ka_path+'/dcase_data/audio/dev'\n",
    "wav_eva_fd   = ka_path+'/dcase_data/audio/eva'\n",
    "dev_fd       = ka_path+'/dcase_data/features/dev/logmel'\n",
    "eva_fd       = ka_path+'/dcase_data/features/eva/logmel'\n",
    "label_csv    = '../texts/dcase/dev/meta.txt'\n",
    "txt_eva_path = '../texts/dcase/eva/test.txt'\n",
    "new_p        = '../texts/dcase/eva/evaluate.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Labels**: We give the names of all the labels in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [ 'bus', 'cafe/restaurant', 'car', 'city_center', 'forest_path', 'grocery_store', 'home', 'beach', \n",
    "            'library', 'metro_station', 'office', 'residential_area', 'train', 'tram', 'park' ]\n",
    "lb_to_id = { lb:id for id, lb in enumerate(labels) }\n",
    "id_to_lb = { id:lb for id, lb in enumerate(labels) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "This is where feature extraction takes place. We pass the\n",
    "1. Feature name such as mel, logmel, mfcc.\n",
    "2. Folder containing audios\n",
    "3. Folder where features will be extracted\n",
    "4. A yaml file containing parameters for features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extraction complete!\n",
      "Feature found\n"
     ]
    }
   ],
   "source": [
    "aud_audio.extract(feature, wav_dev_fd, dev_fd,'example.yaml')\n",
    "#aud_audio.extract('cqt', wav_eva_fd, eva_fd,'example.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "We define all model parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prep='dev'               # Which mode to use(String) Can be dev or eval.\n",
    "save_model=False          # True when we want to save the model with weights.\n",
    "folds=4\n",
    "#Parameters that are passed to the model.\n",
    "model_type='Functional'   # Type of model Can be Dynamic or Functional or Static\n",
    "model='CNN'               # Name of model(String) Can be DNN or CNN\n",
    "feature=\"logmel\"          # Name of feature(String) Can be mel logmel cqt mfcc zcr \n",
    "#Works only for Functional\n",
    "dropout1=0.1             # 1st Dropout(Float) \n",
    "act1='relu'              # 1st Activation(String) \n",
    "act2='relu'              # 2nd Activation(String) \n",
    "act3='softmax'           # 3rd Activation(String) \n",
    "#Works for all Models\n",
    "input_neurons=400      # Number of Neurons\n",
    "epochs=100             # Number of Epochs\n",
    "batchsize=128          # Batch Size\n",
    "num_classes=15         # Number of classes\n",
    "filter_length=3        # Size of Filter\n",
    "nb_filter=100          # Number of Filters\n",
    "#Parameters that are passed to the features.\n",
    "agg_num=10             # Number of frames\n",
    "hop=10                 # Hop Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paul=aud_model.Feature(feature=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetAllData(fe_fd, csv_file, agg_num, hop):\n",
    "    \"\"\"\n",
    "    Input: Features folder(String), CSV file(String), agg_num(Integer), hop(Integer).\n",
    "    Output: Loaded features(Numpy Array) and labels(Numpy Array).\n",
    "    Loads all the features saved as pickle files.\n",
    "    \"\"\"\n",
    "    # read csv\n",
    "    with open( csv_file, 'rb') as f:\n",
    "        reader = csv.reader(f)\n",
    "        lis = list(reader)\n",
    "    \n",
    "    # init list\n",
    "    X3d_all = []\n",
    "    y_all = []\n",
    "    i=0\n",
    "    for li in lis:\n",
    "        # load data\n",
    "        [na, lb] = li[0].split('\\t')\n",
    "        na = na.split('/')[1][0:-4]\n",
    "        path = fe_fd + '/' + na + '.f'\n",
    "        try:\n",
    "            X = cPickle.load( open( path, 'rb' ) )\n",
    "        except Exception as e:\n",
    "            print 'Error while parsing',path\n",
    "            continue\n",
    "        # reshape data to (n_block, n_time, n_freq)\n",
    "        i+=1\n",
    "        X3d = aud_model.mat_2d_to_3d( X, agg_num, hop )\n",
    "        X3d_all.append( X3d )\n",
    "        y_all += [ lb_to_id[lb] ] * len( X3d )\n",
    "    \n",
    "    print \"Features loaded\",i                \n",
    "    print 'All files loaded successfully'\n",
    "    # concatenate list to array\n",
    "    X3d_all = np.concatenate( X3d_all )\n",
    "    y_all = np.array( y_all )\n",
    "    \n",
    "    return X3d_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(md,csv_file,new_p,model):\n",
    "    # load name of wavs to be classified\n",
    "    with open( csv_file, 'rb') as f:\n",
    "        reader = csv.reader(f)\n",
    "        lis = list(reader)\n",
    "    \n",
    "    # do classification for each file\n",
    "    names = []\n",
    "    pred_lbs = []\n",
    "    \n",
    "    for li in lis:\n",
    "        names.append( li[0] )\n",
    "        na = li[0][6:-4]\n",
    "        #audio evaluation name\n",
    "        fe_path = eva_fd + '/' + na + '.f'\n",
    "        X0 = cPickle.load( open( fe_path, 'rb' ) )\n",
    "        X0 = aud_model.mat_2d_to_3d( X0, agg_num, hop )\n",
    "        \n",
    "        X0 = aud_model.mat_3d_to_nd(model,X0)\n",
    "    \n",
    "        # predict\n",
    "        p_y_preds = md.predict(X0)        # probability, size: (n_block,label)\n",
    "        preds = np.argmax( p_y_preds, axis=-1 )     # size: (n_block)\n",
    "        b = scipy.stats.mode(preds)\n",
    "        pred = int( b[0] )\n",
    "        pred_lbs.append( id_to_lb[ pred ] )\n",
    "    \n",
    "    pred = []    \n",
    "    # write out result\n",
    "    for i1 in xrange( len( names ) ):\n",
    "        fname = names[i1] + '\\t' + pred_lbs[i1] + '\\n' \n",
    "        pred.append(fname)\n",
    "        \n",
    "    print 'write out finished!'\n",
    "    truth = open(new_p,'r').readlines()\n",
    "    pred = [i.split('\\t')[1].split('\\n')[0]for i in pred]\n",
    "    truth = [i.split('\\t')[1]for i in truth]\n",
    "    pred.sort()\n",
    "    truth.sort()\n",
    "    return truth,pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features loaded 1170\n",
      "All files loaded successfully\n"
     ]
    }
   ],
   "source": [
    "tr_X, tr_y = GetAllData( dev_fd, label_csv, agg_num, hop )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150930L, 10L, 40L)\n",
      "(150930L,)\n"
     ]
    }
   ],
   "source": [
    "print(tr_X.shape)\n",
    "print(tr_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150930L, 1L, 10L, 40L)\n"
     ]
    }
   ],
   "source": [
    "tr_X=aud_model.mat_3d_to_nd(model,tr_X)\n",
    "print(tr_X.shape)\n",
    "dimx=tr_X.shape[-2]\n",
    "dimy=tr_X.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prep=='dev':\n",
    "    cross_validation=True\n",
    "else:\n",
    "    cross_validation=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "miz=aud_model.Functional_Model(input_neurons=input_neurons,cross_validation=cross_validation,dropout1=dropout1,\n",
    "    act1=act1,act2=act2,act3=act3,nb_filter = nb_filter, filter_length=filter_length,\n",
    "    epochs=epochs,batchsize=batchsize,num_classes=num_classes,\n",
    "    model=model,agg_num=agg_num,hop=hop,dimx=dimx,dimy=dimy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development Mode\n",
      "Activation 1 relu 2 relu 3 softmax\n",
      "Model CNN\n",
      "Epoch 1/100\n",
      " 28160/113197 [======>.......................] - ETA: 133s - loss: 2.7284 - acc: 0.1129"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-1e04baf0a5e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mlrmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmiz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmiz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m#make prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1507\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\keras\\backend\\theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\theano\\gof\\op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(1523)\n",
    "if cross_validation:\n",
    "    kf = KFold(len(tr_X),folds,shuffle=True,random_state=42)\n",
    "    results=[]    \n",
    "    for train_indices, test_indices in kf:\n",
    "        train_x = [tr_X[ii] for ii in train_indices]\n",
    "        train_y = [tr_y[ii] for ii in train_indices]\n",
    "        test_x  = [tr_X[ii] for ii in test_indices]\n",
    "        test_y  = [tr_y[ii] for ii in test_indices]\n",
    "        train_y = to_categorical(train_y,num_classes=len(labels))\n",
    "        test_y = to_categorical(test_y,num_classes=len(labels)) \n",
    "        \n",
    "        train_x=np.array(train_x)\n",
    "        train_y=np.array(train_y)\n",
    "        test_x=np.array(test_x)\n",
    "        test_y=np.array(test_y)\n",
    "        print \"Development Mode\"\n",
    "\n",
    "        #get compiled model\n",
    "        lrmodel=miz.prepare_model()\n",
    "\n",
    "        if lrmodel is None:\n",
    "            print \"If you have used Dynamic Model, make sure you pass correct parameters\"\n",
    "            raise SystemExit\n",
    "        #fit the model\n",
    "        lrmodel.fit(train_x,train_y,batch_size=miz.batchsize,epochs=miz.epochs,verbose=1)\n",
    "        \n",
    "        #make prediction\n",
    "        pred=lrmodel.predict(test_x, batch_size=32, verbose=2)\n",
    "\n",
    "        pred = [ii.argmax()for ii in pred]\n",
    "        test_y = [ii.argmax()for ii in test_y]\n",
    "\n",
    "        results.append(accuracy_score(pred,test_y))\n",
    "        print accuracy_score(pred,test_y)\n",
    "        jj=str(set(list(test_y)))\n",
    "        print \"Unique in test_y\",jj\n",
    "    print \"Results: \" + str( np.array(results).mean() )\n",
    "else:\n",
    "    train_x=np.array(tr_X)\n",
    "    train_y=np.array(tr_y)\n",
    "    print \"Evaluation mode\"\n",
    "    lrmodel=miz.prepare_model()\n",
    "    train_y = to_categorical(train_y,num_classes=len(labels))\n",
    "        \n",
    "    #fit the model\n",
    "    lrmodel.fit(train_x,train_y,batch_size=miz.batchsize,epochs=epochs,verbose=2)\n",
    "    if save_model:\n",
    "        lrmodel.save(modelx)\n",
    "        lrmodel = load_model(modelx)\n",
    "\n",
    "    truth,pred=test(lrmodel,txt_eva_path,new_p,model)\n",
    "\n",
    "    acc=aud_model.calculate_accuracy(truth,pred)\n",
    "    print \"Accuracy %.2f prcnt\"%acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
